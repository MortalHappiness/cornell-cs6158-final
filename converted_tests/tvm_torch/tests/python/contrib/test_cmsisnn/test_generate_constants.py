# Licensed to the Apache Software Foundation (ASF) under one
# or more contributor license agreements.  See the NOTICE file
# distributed with this work for additional information
# regarding copyright ownership.  The ASF licenses this file
# to you under the Apache License, Version 2.0 (the
# "License"); you may not use this file except in compliance
# with the License.  You may obtain a copy of the License at
#
#   http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing,
# software distributed under the License is distributed on an
# "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY
# KIND, either express or implied.  See the License for the
# specific language governing permissions and limitations
# under the License.

"""CMSIS-NN integration tests: generate_constants pass"""
import math
import numpy as np
import pytest
import torch
import torch.nn.functional as F
import torch.testing as testing # Placeholder if assert_allclose is needed for other tests

# TODO: The original TVM test `test_generate_constants.py` is an integration test
# for TVM's CMSIS-NN backend. It verifies that TVM's internal graph transformation
# pass (`cmsisnn.partition_for_cmsisnn`) correctly embeds specific quantization
# parameters as `relay.expr.Constant` nodes in the generated Relay IR.
# This entire workflow is fundamentally tied to the TVM compilation stack and its
# Intermediate Representation (IR) and has no direct conceptual or API equivalent
# in PyTorch. Therefore, the core test logic cannot be meaningfully converted.
# The test functions and helper utilities are adapted to be valid Python but will
# either skip or raise NotImplementedError to indicate their non-translatability.


def quantize_scale(scale):
    """Calculates multiplier and shift for quantization."""
    multiplier, shift = math.frexp(scale)
    multiplier_q31 = round(multiplier * (1 << 31))
    return multiplier_q31, shift


# Helper functions from original TVM .utils, marked as not convertible for this context.
# In a true PyTorch QNN test, these would be replaced by PyTorch's quantization API usage.
def make_module(model_expr):
    """Creates a dummy module to satisfy call signature, as TVM IRModule is not convertible."""
    pytest.skip("TVM helper `make_module` creates a TVM IRModule and cannot be directly converted to PyTorch.")
    return None # Unreachable


def get_same_padding(input_shape_hw, kernel_shape_hw, dilation_hw, strides_hw):
    """Calculates 'SAME' padding values, agnostic to TVM/PyTorch for this math logic."""
    out_height = math.ceil(input_shape_hw[0] / strides_hw[0])
    out_width = math.ceil(input_shape_hw[1] / strides_hw[1])

    pad_along_height = max(
        0,
        (out_height - 1) * strides_hw[0]
        + kernel_shape_hw[0]
        + (kernel_shape_hw[0] - 1) * (dilation_hw[0] - 1)
        - input_shape_hw[0],
    )
    pad_along_width = max(
        0,
        (out_width - 1) * strides_hw[1]
        + kernel_shape_hw[1]
        + (kernel_shape_hw[1] - 1) * (dilation_hw[1] - 1)
        - input_shape_hw[1],
    )

    pad_top = pad_along_height // 2
    pad_bottom = pad_along_height - pad_top
    pad_left = pad_along_width // 2
    pad_right = pad_along_width - pad_left
    return (pad_top, pad_left, pad_bottom, pad_right) # Returns as (top, left, bottom, right)


def get_conv2d_qnn_params(*args, **kwargs):
    """TVM-specific helper for QNN parameter calculation; no direct PyTorch equivalent."""
    pytest.skip("TVM-specific helper `get_conv2d_qnn_params` for Relay QNN parameters has no direct PyTorch equivalent.")
    return 1.0, 0 # Dummy return, unreachable


def make_qnn_relu(x, relu_type, output_scale, output_zero_point, dtype):
    """TVM-specific helper for QNN ReLU; no direct PyTorch equivalent for its role in graph construction."""
    pytest.skip("TVM helper `make_qnn_relu` constructs a Relay QNN operator and cannot be directly translated to PyTorch.")
    return None # Dummy return, unreachable


class CheckGeneratedConstants:
    """
    TODO: This class is designed to traverse and inspect TVM Relay Intermediate Representation (IR).
    It checks for specific `relay.expr.Constant` nodes generated by a TVM-specific backend partitioning pass.
    There is no direct equivalent mechanism or API in PyTorch to perform this type of IR inspection.
    Therefore, this class is marked as non-translatable.
    """

    def __init__(self, enable_bias, multiplier, shift):
        self.num_constant_args_ = 0
        self.enable_bias_ = enable_bias
        self.multiplier_ = multiplier
        self.shift_ = shift
        self._is_tvm_specific = True # Flag to indicate non-translatable nature

    def visit_call(self, call):
        raise NotImplementedError(
            "`CheckGeneratedConstants.visit_call` is TVM-specific and cannot be translated to PyTorch."
        )

    def visit_function(self, func):
        raise NotImplementedError(
            "`CheckGeneratedConstants.visit_function` is TVM-specific and cannot be translated to PyTorch."
        )


def make_model(
    shape,
    kernel_shape,
    input_zero_point,
    input_scale,
    kernel_zero_point,
    kernel_scale,
    output_zero_point,
    output_scale,
    padding,
    strides,
    dilation,
    groups,
    dtype,
    kernel_dtype,
    out_channels,
    weight_format,
    enable_bias,
    relu_type,
):
    """
    TODO: This function constructs a TVM Relay graph using `relay.var`, `relay.qnn.op.conv2d`, etc.
    This graph is then processed by TVM's CMSIS-NN partitioner.
    Constructing a PyTorch model with `torch.nn.Module` or `torch.nn.functional` calls would be
    the PyTorch equivalent for *performing* the computation, but it would not yield an IR structure
    that can be inspected in the same manner as the original TVM test aims to do (i.e., verifying
    internal constant generation by a TVM backend pass). Thus, this function is marked as not convertible.
    """
    pytest.skip("`make_model` constructs a TVM Relay graph and cannot be directly translated to PyTorch in this context.")
    return None, None # Dummy return, unreachable


@pytest.mark.skip(
    reason="This test verifies TVM Relay graph transformations specific to the CMSIS-NN backend "
    "and its internal constant generation. It cannot be directly converted to a PyTorch test."
)
@pytest.mark.parametrize("enable_bias", [True, False])
@pytest.mark.parametrize(
    "input_zero_point, input_scale, kernel_scale, out_channels",
    [(10, 0.0128, [0.11, 0.22], 2), (-64, 1, [1, 0.0256, 1.37], 3)],
)
def test_op_int8(
    enable_bias,
    input_zero_point,
    input_scale,
    kernel_scale,
    out_channels,
):
    """
    TODO: The original TVM test verifies that the CMSIS-NN partitioning pass in TVM
    correctly generates and inserts quantization-related constants into the Relay IR.
    This is a highly TVM-specific integration test for its backend infrastructure.
    There is no direct or equivalent way to formulate this test for PyTorch/TorchInductor
    as PyTorch's quantization and backend integration model is different.
    The test is skipped.
    """
    pass
